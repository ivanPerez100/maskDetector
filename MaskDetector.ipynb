{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0e941ade125eeefc29ec32e2b6023ed6dca9637b19a0f200811155bd0ad6e9d8c",
   "display_name": "Python 3.8.8 64-bit ('tf': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e941ade125eeefc29ec32e2b6023ed6dca9637b19a0f200811155bd0ad6e9d8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Installation\n",
    "\n",
    "First, we need to install Tensorflow2 Object Detection API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports\n",
    "Now we have to import all of the packages that we are going to use "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'models' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "! rm -rf /models\n",
    "\n",
    "# Close the tensorflow Models\n",
    "! git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " already satisfied: contextlib2 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from object-detection==0.1) (0.6.0.post1)\n",
      "Requirement already satisfied: tf-slim in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: six in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from object-detection==0.1) (1.15.0)\n",
      "Requirement already satisfied: pycocotools in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from object-detection==0.1) (2.0.2)\n",
      "Requirement already satisfied: lvis in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from object-detection==0.1) (0.5.3)\n",
      "Requirement already satisfied: scipy in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from object-detection==0.1) (1.6.2)\n",
      "Requirement already satisfied: pandas in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from object-detection==0.1) (1.2.4)\n",
      "Requirement already satisfied: tf-models-official in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from object-detection==0.1) (2.4.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (2.6.0)\n",
      "Requirement already satisfied: pytz>=2018.3 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (2021.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (3.7.4.3)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (2.8.1)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (0.18.2)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (1.34.1)\n",
      "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (1.7)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (1.4.2)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (3.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (2.25.1)\n",
      "Requirement already satisfied: pyarrow<3.0.0,>=0.15.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (2.0.0)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (2.0.0)\n",
      "Requirement already satisfied: numpy<1.20.0,>=1.14.3 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (1.19.5)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (4.1.3)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (0.3.1.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from apache-beam->object-detection==0.1) (3.15.8)\n",
      "Requirement already satisfied: docopt in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
      "Requirement already satisfied: pbr>=0.11 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from mock<3.0.0,>=1.0.1->apache-beam->object-detection==0.1) (5.5.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.2.8)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from pydot<2,>=1.2.0->apache-beam->object-detection==0.1) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2020.12.5)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from lvis->object-detection==0.1) (4.5.1.48)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from lvis->object-detection==0.1) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from lvis->object-detection==0.1) (1.3.1)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from pycocotools->object-detection==0.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (4.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (5.4.1)\n",
      "Requirement already satisfied: dataclasses in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (0.6)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (2.2.0)\n",
      "Requirement already satisfied: seqeval in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (1.2.2)\n",
      "Collecting tensorflow>=2.4.0\n",
      "  Using cached tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n",
      "Requirement already satisfied: gin-config in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (0.4.0)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (2.13.1)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (8.0.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (5.8.0)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (0.5.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (0.12.0)\n",
      "Requirement already satisfied: opencv-python-headless in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (4.5.1.48)\n",
      "Requirement already satisfied: tensorflow-addons in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (0.12.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (0.1.91)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tf-models-official->object-detection==0.1) (1.5.12)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (3.0.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.26.3)\n",
      "Requirement already satisfied: google-auth<2dev,>=1.16.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.29.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (0.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.53.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (20.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (4.2.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.18.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.6.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.2.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (2.20)\n",
      "Requirement already satisfied: tqdm in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.60.0)\n",
      "Requirement already satisfied: python-slugify in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.0.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.6.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (0.12.0)\n",
      "Collecting grpcio<2,>=1.29.0\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (2.4.0)\n",
      "Collecting h5py~=2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (3.3.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.12.1)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (2.5.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (0.36.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (0.2.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (0.6.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (3.3.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (3.1.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official->object-detection==0.1) (0.1.6)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official->object-detection==0.1) (1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from seqeval->tf-models-official->object-detection==0.1) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official->object-detection==0.1) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official->object-detection==0.1) (1.0.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow-addons->tf-models-official->object-detection==0.1) (2.12.0)\n",
      "Requirement already satisfied: promise in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (2.3)\n",
      "Requirement already satisfied: importlib-resources in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (5.1.2)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (0.30.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (20.3.0)\n",
      "Building wheels for collected packages: object-detection\n",
      "  Building wheel for object-detection (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1644045 sha256=dc2c798609c94c024907f02481641c04255e56827f96825bebbf9fa8a75735d2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j7kt1j7d/wheels/39/08/a3/b88e9bcb71b4399f7bfaf9bd5951af985ee3b6da97ee540267\n",
      "Successfully built object-detection\n",
      "Installing collected packages: grpcio, h5py, gast, tensorflow, object-detection\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: object-detection\n",
      "    Found existing installation: object-detection 0.1\n",
      "    Uninstalling object-detection-0.1:\n",
      "      Successfully uninstalled object-detection-0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-nightly-gpu 2.5.0.dev20210302 requires gast==0.4.0, but you have gast 0.3.3 which is incompatible.\n",
      "tf-nightly-gpu 2.5.0.dev20210302 requires grpcio~=1.34.0, but you have grpcio 1.32.0 which is incompatible.\n",
      "tf-nightly-gpu 2.5.0.dev20210302 requires h5py~=3.1.0, but you have h5py 2.10.0 which is incompatible.\u001b[0m\n",
      "Successfully installed gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 object-detection-0.1 tensorflow-2.4.1\n"
     ]
    }
   ],
   "source": [
    "# We also have to install the Object Detection API\n",
    "! cd models/research/ && protoc object_detection/protos/*proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "# import tensorflow_hub as hub\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import csv\n",
    "import random\n",
    "from numpy import asarray\n",
    "import glob\n",
    "import imageio\n",
    "import io\n",
    "import scipy.misc\n",
    "import PIL.Image\n",
    "import tarfile\n",
    "import six.moves.urllib as urllib\n",
    "import re\n",
    "from google.protobuf import text_format\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "# from d2l import tensorflow as d2l\n",
    "from mxnet import image, nd, contrib\n",
    "\n",
    "# For reading xml files\n",
    "import bs4\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os, fnmatch\n",
    "from natsort import os_sorted\n",
    "\n",
    "# import the label map utility module\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "# import module for reading and updating configuration files.\n",
    "from object_detection.utils import config_util\n",
    "\n",
    "# import module for visualization. use the alias `viz_utils`\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "# import module for building the detection model\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "from object_detection.utils import dataset_util"
   ]
  },
  {
   "source": [
    "Since we are going to be using a GPU we need to make sure that Tensorflow is actually detecting our GPU and if it is detecting it, we need to set memory growth to true since sometimes we came across memory issues where we ran out of memory when training.\n",
    "\n",
    "Note: It is very important that you have the correct environment set up when trying to use GPU. This means to have the approriate Drivers, CUDA-Toolkit, CUDNN, and Tensorflow-GPU version installed. If any of these aren't compatible with one another, then there are going to be a lot of errors later on when you try to train the model on the GPU. And it would be possible to instead train on the CPU but that would make training time way slower than when training on a GPU."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(\"NOPE\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Global Variables that would be useful later on\n",
    "NUM_TRAIN_STEPS = 50000\n",
    "MODEL_NAME = \"faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\"\n",
    "\n",
    "CHECKPOINT_PATH = \"./content/checkpoint\"\n",
    "OUTPUT_PATH = \"./content/output\"\n",
    "EXPORTED_PATH = \"./content/exported\"\n",
    "DATA_PATH = \"./content/data\"\n",
    "IMAGE_PATH = \"./content/data/images\"\n",
    "\n",
    "LABEL_MAP_PATH = os.path.join(DATA_PATH, \"label_map.pbtxt\")\n",
    "TRAIN_RECORD_PATH = os.path.join(DATA_PATH, 'train.record')\n",
    "TEST_RECORD_PATH = os.path.join(DATA_PATH, 'test.record')"
   ]
  },
  {
   "source": [
    "## Parse XML Files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a function that is going to parse the xml files. From these xml files\n",
    "we are going to extract information about the image. In this case, we are going\n",
    "to be extracting the shape of the image and either the single or multiple objects\n",
    "found within the image. We are also going to get the file name so we know which\n",
    "image the information is describing.\n",
    "\n",
    "Input: files ( list of strings )\n",
    "    - This is list which contains the .xml files that we want to parse\n",
    "return: ouput:\n",
    "    - This is going to be a list of lists which is going to contain\n",
    "    the information for this file including:\n",
    "        - File name [ filename tag ]\n",
    "            - A string which represents which image it's annotating\n",
    "        - Image Shape [ size tag ]\n",
    "            - A list which shows the (height, width, depth) of image\n",
    "        - Image Class(s) [ name tag ]\n",
    "            - A string that classifies an image\n",
    "        - Image Bounding Box Annotation(s) [ bndbox tag ] \n",
    "            - A list which tell us the borders for bounding box\n",
    "\n",
    "'''\n",
    "\n",
    "def parse_xml_files(files):\n",
    "\n",
    "    # List that is going to contain our information\n",
    "    output = []\n",
    "\n",
    "    # Iterate through the different files that we want to parse\n",
    "    for f in files:\n",
    "\n",
    "        # We first have to read the XML File\n",
    "        content = []\n",
    "        with open(f, \"r\") as file:\n",
    "            content = file.readlines()\n",
    "            content = \"\".join(content)\n",
    "            bs_content = bs(content, \"lxml\")\n",
    "\n",
    "        # Now that we have read the XML File, we could now get info and save\n",
    "        # to lists that we are going to append to our output\n",
    "        classes = []\n",
    "        classNames = []\n",
    "        bounding_boxes = []\n",
    "\n",
    "        # Save the file name\n",
    "        name = bs_content.find(\"filename\").contents[0]\n",
    "\n",
    "        if name.split(\".\")[1] != 'jpg':\n",
    "            name = name.split(\".\")[0] + '.jpg'\n",
    "\n",
    "        # Save the Size of the image\n",
    "        width = int(bs_content.find(\"size\").find(\"width\").contents[0])\n",
    "        height = int(bs_content.find(\"size\").find(\"height\").contents[0])\n",
    "        depth = int(bs_content.find(\"size\").find(\"depth\").contents[0])\n",
    "        size = (width, height, depth)\n",
    "\n",
    "        # Now we have to iterate through the different objects and get\n",
    "        # the class labels and bounding box boundaries \n",
    "        for obj in bs_content.find_all(\"object\"):\n",
    "\n",
    "            # Save the Class Labels\n",
    "            className = obj.find(\"name\").contents[0]\n",
    "            classNames.append(className)\n",
    "\n",
    "            # Convert Class Label Names to numbers\n",
    "            if className == \"with_mask\":\n",
    "                classes.append(1)\n",
    "            elif className == \"without_mask\":\n",
    "                classes.append(2)\n",
    "            else:\n",
    "                classes.append(3)\n",
    "\n",
    "            # Save the Bounding Box Boundaries\n",
    "            x_min = int(obj.find(\"bndbox\").find(\"xmin\").contents[0])\n",
    "            y_min = int(obj.find(\"bndbox\").find(\"ymin\").contents[0])\n",
    "            x_max = int(obj.find(\"bndbox\").find(\"xmax\").contents[0])\n",
    "            y_max = int(obj.find(\"bndbox\").find(\"ymax\").contents[0])\n",
    "            bounding_boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        output.append([name, size, classNames, bounding_boxes,classes])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a function to annotate our xml files, we could now get a list\n",
    "# of all the files in the annotations directory\n",
    "files = os_sorted(os.listdir(\"content/data/annotations\"))\n",
    "files = [\"content/data/annotations/\" + x for x in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the list of xml files so now we could parse all of these files\n",
    "annotations = parse_xml_files(files)"
   ]
  },
  {
   "source": [
    "## Generate Label Map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"with_mask\", \"without_mask\", \"mask_weared_incorrect\"]\n",
    "\n",
    "# We are going to save our Label Map in label_map.pbtxt\n",
    "with open(LABEL_MAP_PATH, 'w') as f:\n",
    "    for idx, label in enumerate(labels):\n",
    "        f.write('item{\\n')\n",
    "        f.write(\"\\tid: {}\\n\".format(idx+1)) # Indexes have to start at 1\n",
    "        f.write(\"\\tname: '{}'\\n\".format(label))\n",
    "        f.write(\"}\\n\")"
   ]
  },
  {
   "source": [
    "## Create TFRecords\n",
    "\n",
    "Tensorflow Object Detection API expects our data to be in TFRecords format so we have to make sure that we save our data in this format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_records(annotations, label_map, image_path, output):\n",
    "\n",
    "    # Create the TFRecord file\n",
    "    with tf.compat.v1.python_io.TFRecordWriter(output) as writer:\n",
    "\n",
    "        # Iterate through all of the Images\n",
    "        for annotation in annotations:\n",
    "\n",
    "            image_name = annotation[0]\n",
    "\n",
    "            # Get image path\n",
    "            img_path = os.path.join(image_path, image_name)\n",
    "\n",
    "            # Read in the image\n",
    "            with tf.compat.v1.gfile.GFile(img_path, 'rb') as fid:\n",
    "                encoded_jpg = fid.read()\n",
    "\n",
    "            # Open the image and get dimensions\n",
    "            encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "            image = PIL.Image.open(encoded_jpg_io)\n",
    "\n",
    "            width, height = annotation[1][0], annotation[1][1]\n",
    "\n",
    "            # Initialize arrays where we are going to save\n",
    "            xmins, xmaxs, ymins, ymaxs = [], [], [], []\n",
    "            classes_text, classes = [], []\n",
    "\n",
    "            # Loop through all of the annotations for this image\n",
    "            text_labels, boxes = annotation[2], annotation[3]\n",
    "            for text_label, box in zip(text_labels, boxes):\n",
    "                xmins.append(box[0]/width)\n",
    "                ymins.append(box[1]/height)\n",
    "                xmaxs.append(box[2]/width)\n",
    "                ymaxs.append(box[3]/height)\n",
    "                classes_text.append(text_label.encode(\"utf8\"))\n",
    "                classes.append(label_map[text_label])\n",
    "\n",
    "            # Now we can create our TFExample for this image\n",
    "            tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'image/height': dataset_util.int64_feature(height),\n",
    "                'image/width': dataset_util.int64_feature(width),\n",
    "                'image/filename': dataset_util.bytes_feature(image_name.encode(\"utf8\")),\n",
    "                'image/source_id': dataset_util.bytes_feature(image_name.encode(\"utf8\")),\n",
    "                'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "                'image/format': dataset_util.bytes_feature('jpeg'.encode(\"utf8\")),\n",
    "                'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "                'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "                'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "                'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "                'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "                'image/object/class/label': dataset_util.int64_list_feature(classes)\n",
    "            }))\n",
    "            if tf_example:\n",
    "                writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "\n",
    "num_train = int(len(annotations) * 0.8)\n",
    "train_examples = annotations[:num_train]\n",
    "test_examples = annotations[num_train:]\n",
    "\n",
    "create_tf_records(train_examples, label_map, IMAGE_PATH, TRAIN_RECORD_PATH)\n",
    "create_tf_records(test_examples, label_map, IMAGE_PATH, TEST_RECORD_PATH)"
   ]
  },
  {
   "source": [
    "## Download the Base Model\n",
    "\n",
    "Download model from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Configure the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "MODEL_NAME = \"ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8\""
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 32,
   "outputs": []
  },
  {
   "source": [
    "pipeline_skeleton = \"models/research/object_detection/configs/tf2/\" + MODEL_NAME + \".config\"\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_skeleton)\n",
    "\n",
    "num_classes = len(label_map.keys())\n",
    "meta_arch = configs[\"model\"].WhichOneof(\"model\")\n",
    "\n",
    "override_dict = {\n",
    "    'model.{}.num_classes'.format(meta_arch): num_classes,\n",
    "    # 'model.{}.first_stage_box_predictor_conv_hyperparams.regularizer.l2_regularizer.weight'.format(meta_arch): 0.01,\n",
    "    # 'model.{}.second_stage_box_predictor.mask_rcnn_box_predictor.use_dropout'.format(meta_arch): True,\n",
    "    # 'model.{}.second_stage_box_predictor.mask_rcnn_box_predictor.dropout_keep_probability'.format(meta_arch): 0.2,\n",
    "    'train_config.batch_size': 1,\n",
    "    'train_config.fine_tune_checkpoint_type': 'detection',\n",
    "    # 'train_config.use_bfloat16': False,\n",
    "    'train_input_path': TRAIN_RECORD_PATH,\n",
    "    'eval_input_path': TEST_RECORD_PATH,\n",
    "    \"train_config.fine_tune_checkpoint\": os.path.join(CHECKPOINT_PATH, \"ckpt-0\"),\n",
    "    \"train_config.max_number_of_boxes\": 50,\n",
    "    'label_map_path': LABEL_MAP_PATH,\n",
    "}\n",
    "\n",
    "configs = config_util.merge_external_params_with_configs(configs, kwargs_dict=override_dict)\n",
    "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(pipeline_config, DATA_PATH)\n",
    "\n",
    "print(configs)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Maybe overwriting model.ssd.num_classes: 3\n",
      "INFO:tensorflow:Maybe overwriting train_config.batch_size: 1\n",
      "INFO:tensorflow:Maybe overwriting train_config.fine_tune_checkpoint_type: detection\n",
      "INFO:tensorflow:Maybe overwriting train_input_path: ./content/data/train.record\n",
      "INFO:tensorflow:Maybe overwriting eval_input_path: ./content/data/test.record\n",
      "INFO:tensorflow:Maybe overwriting train_config.fine_tune_checkpoint: ./content/checkpoint/ckpt-0\n",
      "INFO:tensorflow:Maybe overwriting train_config.max_number_of_boxes: 50\n",
      "INFO:tensorflow:Maybe overwriting label_map_path: ./content/data/label_map.pbtxt\n",
      "INFO:tensorflow:Writing pipeline config file to ./content/data/pipeline.config\n",
      "{'model': ssd {\n",
      "  num_classes: 3\n",
      "  image_resizer {\n",
      "    fixed_shape_resizer {\n",
      "      height: 640\n",
      "      width: 640\n",
      "    }\n",
      "  }\n",
      "  feature_extractor {\n",
      "    type: \"ssd_mobilenet_v1_fpn_keras\"\n",
      "    depth_multiplier: 1.0\n",
      "    min_depth: 16\n",
      "    conv_hyperparams {\n",
      "      regularizer {\n",
      "        l2_regularizer {\n",
      "          weight: 3.9999998989515007e-05\n",
      "        }\n",
      "      }\n",
      "      initializer {\n",
      "        random_normal_initializer {\n",
      "          mean: 0.0\n",
      "          stddev: 0.009999999776482582\n",
      "        }\n",
      "      }\n",
      "      activation: RELU_6\n",
      "      batch_norm {\n",
      "        decay: 0.996999979019165\n",
      "        scale: true\n",
      "        epsilon: 0.0010000000474974513\n",
      "      }\n",
      "    }\n",
      "    override_base_feature_extractor_hyperparams: true\n",
      "    fpn {\n",
      "      min_level: 3\n",
      "      max_level: 7\n",
      "    }\n",
      "  }\n",
      "  box_coder {\n",
      "    faster_rcnn_box_coder {\n",
      "      y_scale: 10.0\n",
      "      x_scale: 10.0\n",
      "      height_scale: 5.0\n",
      "      width_scale: 5.0\n",
      "    }\n",
      "  }\n",
      "  matcher {\n",
      "    argmax_matcher {\n",
      "      matched_threshold: 0.5\n",
      "      unmatched_threshold: 0.5\n",
      "      ignore_thresholds: false\n",
      "      negatives_lower_than_unmatched: true\n",
      "      force_match_for_each_row: true\n",
      "      use_matmul_gather: true\n",
      "    }\n",
      "  }\n",
      "  similarity_calculator {\n",
      "    iou_similarity {\n",
      "    }\n",
      "  }\n",
      "  box_predictor {\n",
      "    weight_shared_convolutional_box_predictor {\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 3.9999998989515007e-05\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          random_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.009999999776482582\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.996999979019165\n",
      "          scale: true\n",
      "          epsilon: 0.0010000000474974513\n",
      "        }\n",
      "      }\n",
      "      depth: 256\n",
      "      num_layers_before_predictor: 4\n",
      "      kernel_size: 3\n",
      "      class_prediction_bias_init: -4.599999904632568\n",
      "    }\n",
      "  }\n",
      "  anchor_generator {\n",
      "    multiscale_anchor_generator {\n",
      "      min_level: 3\n",
      "      max_level: 7\n",
      "      anchor_scale: 4.0\n",
      "      aspect_ratios: 1.0\n",
      "      aspect_ratios: 2.0\n",
      "      aspect_ratios: 0.5\n",
      "      scales_per_octave: 2\n",
      "    }\n",
      "  }\n",
      "  post_processing {\n",
      "    batch_non_max_suppression {\n",
      "      score_threshold: 9.99999993922529e-09\n",
      "      iou_threshold: 0.6000000238418579\n",
      "      max_detections_per_class: 100\n",
      "      max_total_detections: 100\n",
      "    }\n",
      "    score_converter: SIGMOID\n",
      "  }\n",
      "  normalize_loss_by_num_matches: true\n",
      "  loss {\n",
      "    localization_loss {\n",
      "      weighted_smooth_l1 {\n",
      "      }\n",
      "    }\n",
      "    classification_loss {\n",
      "      weighted_sigmoid_focal {\n",
      "        gamma: 2.0\n",
      "        alpha: 0.25\n",
      "      }\n",
      "    }\n",
      "    classification_weight: 1.0\n",
      "    localization_weight: 1.0\n",
      "  }\n",
      "  encode_background_as_zeros: true\n",
      "  normalize_loc_loss_by_codesize: true\n",
      "  inplace_batchnorm_update: true\n",
      "  freeze_batchnorm: false\n",
      "}\n",
      ", 'train_config': batch_size: 1\n",
      "data_augmentation_options {\n",
      "  random_horizontal_flip {\n",
      "  }\n",
      "}\n",
      "data_augmentation_options {\n",
      "  random_crop_image {\n",
      "    min_object_covered: 0.0\n",
      "    min_aspect_ratio: 0.75\n",
      "    max_aspect_ratio: 3.0\n",
      "    min_area: 0.75\n",
      "    max_area: 1.0\n",
      "    overlap_thresh: 0.0\n",
      "  }\n",
      "}\n",
      "sync_replicas: true\n",
      "optimizer {\n",
      "  momentum_optimizer {\n",
      "    learning_rate {\n",
      "      cosine_decay_learning_rate {\n",
      "        learning_rate_base: 0.03999999910593033\n",
      "        total_steps: 25000\n",
      "        warmup_learning_rate: 0.013333000242710114\n",
      "        warmup_steps: 2000\n",
      "      }\n",
      "    }\n",
      "    momentum_optimizer_value: 0.8999999761581421\n",
      "  }\n",
      "  use_moving_average: false\n",
      "}\n",
      "fine_tune_checkpoint: \"./content/checkpoint/ckpt-0\"\n",
      "num_steps: 25000\n",
      "startup_delay_steps: 0.0\n",
      "replicas_to_aggregate: 8\n",
      "max_number_of_boxes: 50\n",
      "unpad_groundtruth_tensors: false\n",
      "fine_tune_checkpoint_type: \"detection\"\n",
      "fine_tune_checkpoint_version: V2\n",
      ", 'train_input_config': label_map_path: \"./content/data/label_map.pbtxt\"\n",
      "tf_record_input_reader {\n",
      "  input_path: \"./content/data/train.record\"\n",
      "}\n",
      ", 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
      "use_moving_averages: false\n",
      "batch_size: 1\n",
      ", 'eval_input_configs': [label_map_path: \"./content/data/label_map.pbtxt\"\n",
      "shuffle: false\n",
      "num_epochs: 1\n",
      "tf_record_input_reader {\n",
      "  input_path: \"./content/data/test.record\"\n",
      "}\n",
      "], 'eval_input_config': label_map_path: \"./content/data/label_map.pbtxt\"\n",
      "shuffle: false\n",
      "num_epochs: 1\n",
      "tf_record_input_reader {\n",
      "  input_path: \"./content/data/test.record\"\n",
      "}\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "640 model_lib_v2.py:679] Step 18600 per-step time 0.306s loss=0.480\n",
      "INFO:tensorflow:Step 18700 per-step time 0.284s loss=0.507\n",
      "I0505 00:21:50.633971 140559002232640 model_lib_v2.py:679] Step 18700 per-step time 0.284s loss=0.507\n",
      "INFO:tensorflow:Step 18800 per-step time 0.279s loss=0.458\n",
      "I0505 00:22:17.040450 140559002232640 model_lib_v2.py:679] Step 18800 per-step time 0.279s loss=0.458\n",
      "INFO:tensorflow:Step 18900 per-step time 0.300s loss=0.464\n",
      "I0505 00:22:42.898896 140559002232640 model_lib_v2.py:679] Step 18900 per-step time 0.300s loss=0.464\n",
      "INFO:tensorflow:Step 19000 per-step time 0.323s loss=0.436\n",
      "I0505 00:23:09.054235 140559002232640 model_lib_v2.py:679] Step 19000 per-step time 0.323s loss=0.436\n",
      "INFO:tensorflow:Step 19100 per-step time 0.267s loss=0.542\n",
      "I0505 00:23:36.085096 140559002232640 model_lib_v2.py:679] Step 19100 per-step time 0.267s loss=0.542\n",
      "INFO:tensorflow:Step 19200 per-step time 0.268s loss=0.484\n",
      "I0505 00:24:02.610077 140559002232640 model_lib_v2.py:679] Step 19200 per-step time 0.268s loss=0.484\n",
      "INFO:tensorflow:Step 19300 per-step time 0.246s loss=0.472\n",
      "I0505 00:24:28.849006 140559002232640 model_lib_v2.py:679] Step 19300 per-step time 0.246s loss=0.472\n",
      "INFO:tensorflow:Step 19400 per-step time 0.219s loss=0.580\n",
      "I0505 00:24:54.952946 140559002232640 model_lib_v2.py:679] Step 19400 per-step time 0.219s loss=0.580\n",
      "INFO:tensorflow:Step 19500 per-step time 0.246s loss=0.455\n",
      "I0505 00:25:21.405254 140559002232640 model_lib_v2.py:679] Step 19500 per-step time 0.246s loss=0.455\n",
      "INFO:tensorflow:Step 19600 per-step time 0.327s loss=0.450\n",
      "I0505 00:25:47.512500 140559002232640 model_lib_v2.py:679] Step 19600 per-step time 0.327s loss=0.450\n",
      "INFO:tensorflow:Step 19700 per-step time 0.215s loss=0.534\n",
      "I0505 00:26:13.872970 140559002232640 model_lib_v2.py:679] Step 19700 per-step time 0.215s loss=0.534\n",
      "INFO:tensorflow:Step 19800 per-step time 0.237s loss=0.438\n",
      "I0505 00:26:39.864486 140559002232640 model_lib_v2.py:679] Step 19800 per-step time 0.237s loss=0.438\n",
      "INFO:tensorflow:Step 19900 per-step time 0.277s loss=0.437\n",
      "I0505 00:27:06.217336 140559002232640 model_lib_v2.py:679] Step 19900 per-step time 0.277s loss=0.437\n",
      "INFO:tensorflow:Step 20000 per-step time 0.248s loss=0.447\n",
      "I0505 00:27:33.252106 140559002232640 model_lib_v2.py:679] Step 20000 per-step time 0.248s loss=0.447\n",
      "INFO:tensorflow:Step 20100 per-step time 0.245s loss=0.589\n",
      "I0505 00:27:59.863604 140559002232640 model_lib_v2.py:679] Step 20100 per-step time 0.245s loss=0.589\n",
      "INFO:tensorflow:Step 20200 per-step time 0.268s loss=0.421\n",
      "I0505 00:28:25.746533 140559002232640 model_lib_v2.py:679] Step 20200 per-step time 0.268s loss=0.421\n",
      "INFO:tensorflow:Step 20300 per-step time 0.234s loss=0.509\n",
      "I0505 00:28:51.818765 140559002232640 model_lib_v2.py:679] Step 20300 per-step time 0.234s loss=0.509\n",
      "INFO:tensorflow:Step 20400 per-step time 0.271s loss=0.488\n",
      "I0505 00:29:18.392072 140559002232640 model_lib_v2.py:679] Step 20400 per-step time 0.271s loss=0.488\n",
      "INFO:tensorflow:Step 20500 per-step time 0.237s loss=0.581\n",
      "I0505 00:29:44.581914 140559002232640 model_lib_v2.py:679] Step 20500 per-step time 0.237s loss=0.581\n",
      "INFO:tensorflow:Step 20600 per-step time 0.240s loss=0.532\n",
      "I0505 00:30:10.360328 140559002232640 model_lib_v2.py:679] Step 20600 per-step time 0.240s loss=0.532\n",
      "INFO:tensorflow:Step 20700 per-step time 0.293s loss=0.578\n",
      "I0505 00:30:36.207782 140559002232640 model_lib_v2.py:679] Step 20700 per-step time 0.293s loss=0.578\n",
      "INFO:tensorflow:Step 20800 per-step time 0.267s loss=0.446\n",
      "I0505 00:31:02.779785 140559002232640 model_lib_v2.py:679] Step 20800 per-step time 0.267s loss=0.446\n",
      "INFO:tensorflow:Step 20900 per-step time 0.319s loss=0.444\n",
      "I0505 00:31:29.446433 140559002232640 model_lib_v2.py:679] Step 20900 per-step time 0.319s loss=0.444\n",
      "INFO:tensorflow:Step 21000 per-step time 0.278s loss=0.443\n",
      "I0505 00:31:55.550126 140559002232640 model_lib_v2.py:679] Step 21000 per-step time 0.278s loss=0.443\n",
      "INFO:tensorflow:Step 21100 per-step time 0.223s loss=1.172\n",
      "I0505 00:32:23.010662 140559002232640 model_lib_v2.py:679] Step 21100 per-step time 0.223s loss=1.172\n",
      "INFO:tensorflow:Step 21200 per-step time 0.272s loss=0.450\n",
      "I0505 00:32:49.545442 140559002232640 model_lib_v2.py:679] Step 21200 per-step time 0.272s loss=0.450\n",
      "INFO:tensorflow:Step 21300 per-step time 0.247s loss=0.481\n",
      "I0505 00:33:15.735780 140559002232640 model_lib_v2.py:679] Step 21300 per-step time 0.247s loss=0.481\n",
      "INFO:tensorflow:Step 21400 per-step time 0.287s loss=0.441\n",
      "I0505 00:33:41.826198 140559002232640 model_lib_v2.py:679] Step 21400 per-step time 0.287s loss=0.441\n",
      "INFO:tensorflow:Step 21500 per-step time 0.259s loss=0.464\n",
      "I0505 00:34:07.880083 140559002232640 model_lib_v2.py:679] Step 21500 per-step time 0.259s loss=0.464\n",
      "INFO:tensorflow:Step 21600 per-step time 0.253s loss=0.579\n",
      "I0505 00:34:34.499817 140559002232640 model_lib_v2.py:679] Step 21600 per-step time 0.253s loss=0.579\n",
      "INFO:tensorflow:Step 21700 per-step time 0.299s loss=0.430\n",
      "I0505 00:35:00.508333 140559002232640 model_lib_v2.py:679] Step 21700 per-step time 0.299s loss=0.430\n",
      "INFO:tensorflow:Step 21800 per-step time 0.233s loss=0.521\n",
      "I0505 00:35:26.242532 140559002232640 model_lib_v2.py:679] Step 21800 per-step time 0.233s loss=0.521\n",
      "INFO:tensorflow:Step 21900 per-step time 0.265s loss=0.486\n",
      "I0505 00:35:52.252642 140559002232640 model_lib_v2.py:679] Step 21900 per-step time 0.265s loss=0.486\n",
      "INFO:tensorflow:Step 22000 per-step time 0.260s loss=0.404\n",
      "I0505 00:36:18.568039 140559002232640 model_lib_v2.py:679] Step 22000 per-step time 0.260s loss=0.404\n",
      "INFO:tensorflow:Step 22100 per-step time 0.219s loss=0.825\n",
      "I0505 00:36:44.869497 140559002232640 model_lib_v2.py:679] Step 22100 per-step time 0.219s loss=0.825\n",
      "INFO:tensorflow:Step 22200 per-step time 0.271s loss=0.425\n",
      "I0505 00:37:11.234007 140559002232640 model_lib_v2.py:679] Step 22200 per-step time 0.271s loss=0.425\n",
      "INFO:tensorflow:Step 22300 per-step time 0.264s loss=0.436\n",
      "I0505 00:37:37.648361 140559002232640 model_lib_v2.py:679] Step 22300 per-step time 0.264s loss=0.436\n",
      "INFO:tensorflow:Step 22400 per-step time 0.244s loss=0.442\n",
      "I0505 00:38:04.377775 140559002232640 model_lib_v2.py:679] Step 22400 per-step time 0.244s loss=0.442\n",
      "INFO:tensorflow:Step 22500 per-step time 0.278s loss=0.433\n",
      "I0505 00:38:31.321178 140559002232640 model_lib_v2.py:679] Step 22500 per-step time 0.278s loss=0.433\n",
      "INFO:tensorflow:Step 22600 per-step time 0.240s loss=0.501\n",
      "I0505 00:38:57.151404 140559002232640 model_lib_v2.py:679] Step 22600 per-step time 0.240s loss=0.501\n",
      "INFO:tensorflow:Step 22700 per-step time 0.238s loss=0.466\n",
      "I0505 00:39:23.236564 140559002232640 model_lib_v2.py:679] Step 22700 per-step time 0.238s loss=0.466\n",
      "INFO:tensorflow:Step 22800 per-step time 0.329s loss=0.429\n",
      "I0505 00:39:49.815112 140559002232640 model_lib_v2.py:679] Step 22800 per-step time 0.329s loss=0.429\n",
      "INFO:tensorflow:Step 22900 per-step time 0.294s loss=0.426\n",
      "I0505 00:40:16.180777 140559002232640 model_lib_v2.py:679] Step 22900 per-step time 0.294s loss=0.426\n",
      "INFO:tensorflow:Step 23000 per-step time 0.313s loss=0.576\n",
      "I0505 00:40:42.372154 140559002232640 model_lib_v2.py:679] Step 23000 per-step time 0.313s loss=0.576\n",
      "INFO:tensorflow:Step 23100 per-step time 0.255s loss=0.435\n",
      "I0505 00:41:08.470839 140559002232640 model_lib_v2.py:679] Step 23100 per-step time 0.255s loss=0.435\n",
      "INFO:tensorflow:Step 23200 per-step time 0.244s loss=0.507\n",
      "I0505 00:41:34.657068 140559002232640 model_lib_v2.py:679] Step 23200 per-step time 0.244s loss=0.507\n",
      "INFO:tensorflow:Step 23300 per-step time 0.237s loss=0.502\n",
      "I0505 00:42:01.372981 140559002232640 model_lib_v2.py:679] Step 23300 per-step time 0.237s loss=0.502\n",
      "INFO:tensorflow:Step 23400 per-step time 0.301s loss=0.472\n",
      "I0505 00:42:27.578780 140559002232640 model_lib_v2.py:679] Step 23400 per-step time 0.301s loss=0.472\n",
      "INFO:tensorflow:Step 23500 per-step time 0.321s loss=0.448\n",
      "I0505 00:42:53.945166 140559002232640 model_lib_v2.py:679] Step 23500 per-step time 0.321s loss=0.448\n",
      "INFO:tensorflow:Step 23600 per-step time 0.261s loss=0.451\n",
      "I0505 00:43:20.205809 140559002232640 model_lib_v2.py:679] Step 23600 per-step time 0.261s loss=0.451\n",
      "INFO:tensorflow:Step 23700 per-step time 0.253s loss=0.420\n",
      "I0505 00:43:46.467199 140559002232640 model_lib_v2.py:679] Step 23700 per-step time 0.253s loss=0.420\n",
      "INFO:tensorflow:Step 23800 per-step time 0.275s loss=0.396\n",
      "I0505 00:44:12.394206 140559002232640 model_lib_v2.py:679] Step 23800 per-step time 0.275s loss=0.396\n",
      "INFO:tensorflow:Step 23900 per-step time 0.270s loss=0.514\n",
      "I0505 00:44:38.523072 140559002232640 model_lib_v2.py:679] Step 23900 per-step time 0.270s loss=0.514\n",
      "INFO:tensorflow:Step 24000 per-step time 0.227s loss=0.417\n",
      "I0505 00:45:04.742330 140559002232640 model_lib_v2.py:679] Step 24000 per-step time 0.227s loss=0.417\n",
      "INFO:tensorflow:Step 24100 per-step time 0.254s loss=0.518\n",
      "I0505 00:45:31.640169 140559002232640 model_lib_v2.py:679] Step 24100 per-step time 0.254s loss=0.518\n",
      "INFO:tensorflow:Step 24200 per-step time 0.241s loss=0.554\n",
      "I0505 00:45:58.097070 140559002232640 model_lib_v2.py:679] Step 24200 per-step time 0.241s loss=0.554\n",
      "INFO:tensorflow:Step 24300 per-step time 0.260s loss=0.437\n",
      "I0505 00:46:24.292629 140559002232640 model_lib_v2.py:679] Step 24300 per-step time 0.260s loss=0.437\n",
      "INFO:tensorflow:Step 24400 per-step time 0.262s loss=0.483\n",
      "I0505 00:46:50.750533 140559002232640 model_lib_v2.py:679] Step 24400 per-step time 0.262s loss=0.483\n",
      "INFO:tensorflow:Step 24500 per-step time 0.236s loss=0.454\n",
      "I0505 00:47:17.100518 140559002232640 model_lib_v2.py:679] Step 24500 per-step time 0.236s loss=0.454\n",
      "INFO:tensorflow:Step 24600 per-step time 0.312s loss=0.416\n",
      "I0505 00:47:43.314515 140559002232640 model_lib_v2.py:679] Step 24600 per-step time 0.312s loss=0.416\n",
      "INFO:tensorflow:Step 24700 per-step time 0.234s loss=0.463\n",
      "I0505 00:48:09.801429 140559002232640 model_lib_v2.py:679] Step 24700 per-step time 0.234s loss=0.463\n",
      "INFO:tensorflow:Step 24800 per-step time 0.243s loss=0.452\n",
      "I0505 00:48:35.899154 140559002232640 model_lib_v2.py:679] Step 24800 per-step time 0.243s loss=0.452\n",
      "INFO:tensorflow:Step 24900 per-step time 0.231s loss=0.408\n",
      "I0505 00:49:02.293842 140559002232640 model_lib_v2.py:679] Step 24900 per-step time 0.231s loss=0.408\n",
      "INFO:tensorflow:Step 25000 per-step time 0.254s loss=0.597\n",
      "I0505 00:49:28.999321 140559002232640 model_lib_v2.py:679] Step 25000 per-step time 0.254s loss=0.597\n",
      "INFO:tensorflow:Step 25100 per-step time 0.279s loss=0.438\n",
      "I0505 00:49:55.800156 140559002232640 model_lib_v2.py:679] Step 25100 per-step time 0.279s loss=0.438\n",
      "INFO:tensorflow:Step 25200 per-step time 0.243s loss=0.497\n",
      "I0505 00:50:21.982790 140559002232640 model_lib_v2.py:679] Step 25200 per-step time 0.243s loss=0.497\n",
      "INFO:tensorflow:Step 25300 per-step time 0.287s loss=0.383\n",
      "I0505 00:50:48.444549 140559002232640 model_lib_v2.py:679] Step 25300 per-step time 0.287s loss=0.383\n",
      "INFO:tensorflow:Step 25400 per-step time 0.244s loss=0.478\n",
      "I0505 00:51:14.695086 140559002232640 model_lib_v2.py:679] Step 25400 per-step time 0.244s loss=0.478\n",
      "INFO:tensorflow:Step 25500 per-step time 0.243s loss=0.383\n",
      "I0505 00:51:40.994054 140559002232640 model_lib_v2.py:679] Step 25500 per-step time 0.243s loss=0.383\n",
      "INFO:tensorflow:Step 25600 per-step time 0.225s loss=0.525\n",
      "I0505 00:52:07.418764 140559002232640 model_lib_v2.py:679] Step 25600 per-step time 0.225s loss=0.525\n",
      "INFO:tensorflow:Step 25700 per-step time 0.241s loss=0.374\n",
      "I0505 00:52:33.621566 140559002232640 model_lib_v2.py:679] Step 25700 per-step time 0.241s loss=0.374\n",
      "INFO:tensorflow:Step 25800 per-step time 0.266s loss=0.503\n",
      "I0505 00:52:59.853471 140559002232640 model_lib_v2.py:679] Step 25800 per-step time 0.266s loss=0.503\n",
      "INFO:tensorflow:Step 25900 per-step time 0.267s loss=0.422\n",
      "I0505 00:53:26.332363 140559002232640 model_lib_v2.py:679] Step 25900 per-step time 0.267s loss=0.422\n",
      "INFO:tensorflow:Step 26000 per-step time 0.230s loss=0.529\n",
      "I0505 00:53:52.713539 140559002232640 model_lib_v2.py:679] Step 26000 per-step time 0.230s loss=0.529\n",
      "INFO:tensorflow:Step 26100 per-step time 0.234s loss=0.515\n",
      "I0505 00:54:19.261020 140559002232640 model_lib_v2.py:679] Step 26100 per-step time 0.234s loss=0.515\n",
      "INFO:tensorflow:Step 26200 per-step time 0.244s loss=0.537\n",
      "I0505 00:54:45.979331 140559002232640 model_lib_v2.py:679] Step 26200 per-step time 0.244s loss=0.537\n",
      "INFO:tensorflow:Step 26300 per-step time 0.280s loss=0.388\n",
      "I0505 00:55:11.968056 140559002232640 model_lib_v2.py:679] Step 26300 per-step time 0.280s loss=0.388\n",
      "INFO:tensorflow:Step 26400 per-step time 0.235s loss=0.397\n",
      "I0505 00:55:38.476073 140559002232640 model_lib_v2.py:679] Step 26400 per-step time 0.235s loss=0.397\n",
      "INFO:tensorflow:Step 26500 per-step time 0.231s loss=0.727\n",
      "I0505 00:56:04.575349 140559002232640 model_lib_v2.py:679] Step 26500 per-step time 0.231s loss=0.727\n",
      "INFO:tensorflow:Step 26600 per-step time 0.276s loss=0.412\n",
      "I0505 00:56:31.011482 140559002232640 model_lib_v2.py:679] Step 26600 per-step time 0.276s loss=0.412\n",
      "INFO:tensorflow:Step 26700 per-step time 0.315s loss=0.406\n",
      "I0505 00:56:56.825145 140559002232640 model_lib_v2.py:679] Step 26700 per-step time 0.315s loss=0.406\n",
      "INFO:tensorflow:Step 26800 per-step time 0.254s loss=0.445\n",
      "I0505 00:57:23.451556 140559002232640 model_lib_v2.py:679] Step 26800 per-step time 0.254s loss=0.445\n",
      "INFO:tensorflow:Step 26900 per-step time 0.282s loss=0.408\n",
      "I0505 00:57:50.160674 140559002232640 model_lib_v2.py:679] Step 26900 per-step time 0.282s loss=0.408\n",
      "INFO:tensorflow:Step 27000 per-step time 0.286s loss=0.458\n",
      "I0505 00:58:16.805487 140559002232640 model_lib_v2.py:679] Step 27000 per-step time 0.286s loss=0.458\n",
      "INFO:tensorflow:Step 27100 per-step time 0.237s loss=0.622\n",
      "I0505 00:58:43.594310 140559002232640 model_lib_v2.py:679] Step 27100 per-step time 0.237s loss=0.622\n",
      "INFO:tensorflow:Step 27200 per-step time 0.241s loss=0.422\n",
      "I0505 00:59:09.759342 140559002232640 model_lib_v2.py:679] Step 27200 per-step time 0.241s loss=0.422\n",
      "INFO:tensorflow:Step 27300 per-step time 0.310s loss=0.478\n",
      "I0505 00:59:35.993056 140559002232640 model_lib_v2.py:679] Step 27300 per-step time 0.310s loss=0.478\n",
      "INFO:tensorflow:Step 27400 per-step time 0.229s loss=0.481\n",
      "I0505 01:00:02.269253 140559002232640 model_lib_v2.py:679] Step 27400 per-step time 0.229s loss=0.481\n",
      "INFO:tensorflow:Step 27500 per-step time 0.283s loss=0.434\n",
      "I0505 01:00:28.501871 140559002232640 model_lib_v2.py:679] Step 27500 per-step time 0.283s loss=0.434\n",
      "INFO:tensorflow:Step 27600 per-step time 0.322s loss=0.382\n",
      "I0505 01:00:54.438080 140559002232640 model_lib_v2.py:679] Step 27600 per-step time 0.322s loss=0.382\n",
      "INFO:tensorflow:Step 27700 per-step time 0.280s loss=0.404\n",
      "I0505 01:01:20.487859 140559002232640 model_lib_v2.py:679] Step 27700 per-step time 0.280s loss=0.404\n",
      "INFO:tensorflow:Step 27800 per-step time 0.235s loss=0.380\n",
      "I0505 01:01:46.932855 140559002232640 model_lib_v2.py:679] Step 27800 per-step time 0.235s loss=0.380\n",
      "INFO:tensorflow:Step 27900 per-step time 0.291s loss=0.410\n",
      "I0505 01:02:13.253190 140559002232640 model_lib_v2.py:679] Step 27900 per-step time 0.291s loss=0.410\n",
      "INFO:tensorflow:Step 28000 per-step time 0.288s loss=0.569\n",
      "I0505 01:02:39.312177 140559002232640 model_lib_v2.py:679] Step 28000 per-step time 0.288s loss=0.569\n",
      "INFO:tensorflow:Step 28100 per-step time 0.291s loss=0.464\n",
      "I0505 01:03:06.127955 140559002232640 model_lib_v2.py:679] Step 28100 per-step time 0.291s loss=0.464\n",
      "INFO:tensorflow:Step 28200 per-step time 0.311s loss=0.380\n",
      "I0505 01:03:32.513126 140559002232640 model_lib_v2.py:679] Step 28200 per-step time 0.311s loss=0.380\n",
      "INFO:tensorflow:Step 28300 per-step time 0.260s loss=0.374\n",
      "I0505 01:03:59.073187 140559002232640 model_lib_v2.py:679] Step 28300 per-step time 0.260s loss=0.374\n",
      "INFO:tensorflow:Step 28400 per-step time 0.320s loss=0.438\n",
      "I0505 01:04:25.821894 140559002232640 model_lib_v2.py:679] Step 28400 per-step time 0.320s loss=0.438\n",
      "INFO:tensorflow:Step 28500 per-step time 0.235s loss=0.541\n",
      "I0505 01:04:52.329945 140559002232640 model_lib_v2.py:679] Step 28500 per-step time 0.235s loss=0.541\n",
      "INFO:tensorflow:Step 28600 per-step time 0.263s loss=0.392\n",
      "I0505 01:05:19.271911 140559002232640 model_lib_v2.py:679] Step 28600 per-step time 0.263s loss=0.392\n",
      "INFO:tensorflow:Step 28700 per-step time 0.265s loss=0.514\n",
      "I0505 01:05:46.218101 140559002232640 model_lib_v2.py:679] Step 28700 per-step time 0.265s loss=0.514\n",
      "INFO:tensorflow:Step 28800 per-step time 0.218s loss=0.423\n",
      "I0505 01:06:12.515596 140559002232640 model_lib_v2.py:679] Step 28800 per-step time 0.218s loss=0.423\n",
      "INFO:tensorflow:Step 28900 per-step time 0.263s loss=0.408\n",
      "I0505 01:06:38.894544 140559002232640 model_lib_v2.py:679] Step 28900 per-step time 0.263s loss=0.408\n",
      "INFO:tensorflow:Step 29000 per-step time 0.249s loss=0.427\n",
      "I0505 01:07:04.880250 140559002232640 model_lib_v2.py:679] Step 29000 per-step time 0.249s loss=0.427\n",
      "INFO:tensorflow:Step 29100 per-step time 0.272s loss=0.389\n",
      "I0505 01:07:32.165452 140559002232640 model_lib_v2.py:679] Step 29100 per-step time 0.272s loss=0.389\n",
      "INFO:tensorflow:Step 29200 per-step time 0.225s loss=0.404\n",
      "I0505 01:07:58.381991 140559002232640 model_lib_v2.py:679] Step 29200 per-step time 0.225s loss=0.404\n",
      "INFO:tensorflow:Step 29300 per-step time 0.315s loss=0.394\n",
      "I0505 01:08:25.236044 140559002232640 model_lib_v2.py:679] Step 29300 per-step time 0.315s loss=0.394\n",
      "INFO:tensorflow:Step 29400 per-step time 0.281s loss=0.459\n",
      "I0505 01:08:51.493210 140559002232640 model_lib_v2.py:679] Step 29400 per-step time 0.281s loss=0.459\n",
      "INFO:tensorflow:Step 29500 per-step time 0.278s loss=0.492\n",
      "I0505 01:09:17.456095 140559002232640 model_lib_v2.py:679] Step 29500 per-step time 0.278s loss=0.492\n",
      "INFO:tensorflow:Step 29600 per-step time 0.232s loss=0.448\n",
      "I0505 01:09:44.039913 140559002232640 model_lib_v2.py:679] Step 29600 per-step time 0.232s loss=0.448\n",
      "INFO:tensorflow:Step 29700 per-step time 0.229s loss=0.385\n",
      "I0505 01:10:10.095358 140559002232640 model_lib_v2.py:679] Step 29700 per-step time 0.229s loss=0.385\n",
      "INFO:tensorflow:Step 29800 per-step time 0.320s loss=0.409\n",
      "I0505 01:10:36.344969 140559002232640 model_lib_v2.py:679] Step 29800 per-step time 0.320s loss=0.409\n",
      "INFO:tensorflow:Step 29900 per-step time 0.242s loss=0.401\n",
      "I0505 01:11:02.664345 140559002232640 model_lib_v2.py:679] Step 29900 per-step time 0.242s loss=0.401\n",
      "INFO:tensorflow:Step 30000 per-step time 0.318s loss=0.408\n",
      "I0505 01:11:29.022957 140559002232640 model_lib_v2.py:679] Step 30000 per-step time 0.318s loss=0.408\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"models/research/object_detection/model_main_tf2.py\", line 113, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/absl/app.py\", line 303, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"models/research/object_detection/model_main_tf2.py\", line 104, in main\n",
      "    model_lib_v2.train_loop(\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/object_detection/model_lib_v2.py\", line 667, in train_loop\n",
      "    loss = _dist_train_step(train_input_iter)\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 867, in __call__\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 895, in _call\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3018, in __call__\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1960, in _call_flat\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 591, in call\n",
      "  File \"/home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# ## Start Training\n",
    "# ! rm -rf $OUTPUT_PATH\n",
    "\n",
    "# ! python models/research/object_detection/model_main_tf2.py \\\n",
    "#     --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "#     --model_dir=$OUTPUT_PATH \\\n",
    "#     --num_train_steps=$NUM_TRAIN_STEPS \\\n",
    "#     --num_eval_steps=100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! python models/research/object_detection/model_main_tf2.py \\\n",
    "#     --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "#     --model_dir=$OUTPUT_PATH \\\n",
    "#     --checkpoint_dir=$OUTPUT_PATH"
   ]
  },
  {
   "source": [
    "## Export Inference Graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'./content/output/ckpt-31'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "TRAINED_CHECKPOINT_PREFIX = os.path.join(OUTPUT_PATH, \"ckpt-31\")\n",
    "TRAINED_CHECKPOINT_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-05-05 01:17:29.366640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-05-05 01:17:30.690983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-05-05 01:17:30.716560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-05 01:17:30.716962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1770] Found device 0 with properties: \n",
      "pciBusID: 0000:2d:00.0 name: NVIDIA GeForce GTX 1650 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 14 deviceMemorySize: 3.81GiB deviceMemoryBandwidth: 178.84GiB/s\n",
      "2021-05-05 01:17:30.716978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-05-05 01:17:30.718219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-05-05 01:17:30.718248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-05-05 01:17:30.718793: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-05-05 01:17:30.718910: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-05-05 01:17:30.720179: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-05-05 01:17:30.720488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-05-05 01:17:30.720557: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-05-05 01:17:30.720641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-05 01:17:30.720946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-05 01:17:30.721567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1908] Adding visible gpu devices: 0\n",
      "2021-05-05 01:17:30.721781: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-05-05 01:17:30.722162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-05 01:17:30.722419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1770] Found device 0 with properties: \n",
      "pciBusID: 0000:2d:00.0 name: NVIDIA GeForce GTX 1650 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 14 deviceMemorySize: 3.81GiB deviceMemoryBandwidth: 178.84GiB/s\n",
      "2021-05-05 01:17:30.722468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-05 01:17:30.722740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-05 01:17:30.722974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1908] Adding visible gpu devices: 0\n",
      "2021-05-05 01:17:30.722993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-05-05 01:17:31.033923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-05 01:17:31.033954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 \n",
      "2021-05-05 01:17:31.033963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N \n",
      "2021-05-05 01:17:31.034144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-05 01:17:31.034432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-05 01:17:31.034690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-05 01:17:31.034927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1456] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2038 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:2d:00.0, compute capability: 7.5)\n",
      "WARNING:tensorflow:From /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "W0505 01:17:31.147308 139849829013312 deprecation.py:596] From /home/ivanp/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f30787c0940>, because it is not built.\n",
      "W0505 01:17:40.865048 139849829013312 save_impl.py:77] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f30787c0940>, because it is not built.\n",
      "2021-05-05 01:17:43.799864: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "W0505 01:17:51.780647 139849829013312 save.py:235] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxPredictor_layer_call_fn while saving (showing 5 of 520). These functions will not be directly callable after loading.\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "W0505 01:17:54.086239 139849829013312 save.py:1221] FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: ./content/exported/saved_model/assets\n",
      "I0505 01:17:54.327552 139849829013312 builder_impl.py:774] Assets written to: ./content/exported/saved_model/assets\n",
      "INFO:tensorflow:Writing pipeline config file to ./content/exported/pipeline.config\n",
      "I0505 01:17:54.756426 139849829013312 config_util.py:253] Writing pipeline config file to ./content/exported/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $EXPORTED_PATH\n",
    "!python models/research/object_detection/exporter_main_v2.py \\\n",
    "  --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "  --trained_checkpoint_dir=$OUTPUT_PATH \\\n",
    "  --output_directory=$EXPORTED_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_detection_model = tf.compat.v2.saved_model.load(\"./content/exported/saved_model/\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image as PImage\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from object_detection.utils import label_map_util\n",
    "import tkinter\n",
    "matplotlib.use('tkagg')"
   ]
  },
  {
   "source": [
    "Below is code that takes in an image and annotates that image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    %matplotlib inline\n",
    "    category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH, use_display_name=True)\n",
    "    image = PImage.open(\"2021-05-01-141258.jpg\")\n",
    "    (im_width, im_height) = image.size\n",
    "    image_np = np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    print((image_np_expanded).shape)\n",
    "    print(type(image_np_expanded))\n",
    "    output = mask_detection_model(image_np_expanded)\n",
    "\n",
    "    image_with_annotations = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_with_annotations,\n",
    "        output[\"detection_boxes\"][0].numpy(),\n",
    "        output['detection_classes'][0].numpy().astype(np.int32),\n",
    "        output['detection_scores'][0].numpy(),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        min_score_thresh=0.6\n",
    "    )\n",
    "    plt.figure()\n",
    "    plt.imshow(image_with_annotations)"
   ]
  },
  {
   "source": [
    "Below is code that instead of taking an image, it would take in a video stream and annotate that video in real time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff76030e280>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(DATA_PATH + \"/pipeline.config\")\n",
    "detection_model = model_builder.build(configs['model'], is_training=False)\n",
    "\n",
    "ckpt = tf.compat.v1.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(OUTPUT_PATH, \"ckpt-31\")).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(-1)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH, use_display_name=True)\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        image_np = np.array(frame)\n",
    "        # input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "        # detections = detect_fn(input_tensor)\n",
    "        detections = mask_detection_model(np.expand_dims(image_np, axis=0))\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key:value[0,:num_detections].numpy() for key,value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "        detections[\"detection_classes\"] = detections[\"detection_classes\"].astype(np.int64)\n",
    "        image_np_with_detections = image_np.copy()\n",
    "\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections[\"detection_boxes\"],#[0].numpy(),\n",
    "            detections['detection_classes'],#[0].numpy().astype(np.int32),\n",
    "            detections['detection_scores'],#[0].numpy(),\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            min_score_thresh = 0.75,\n",
    "            agnostic_mode=False\n",
    "        )\n",
    "\n",
    "        cv2.imshow('frame', image_np_with_detections)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}